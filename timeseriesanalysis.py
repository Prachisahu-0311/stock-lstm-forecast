# -*- coding: utf-8 -*-
"""TimeSeriesAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/140xfLDXIaDWeVH8lDaocVdFG1o9lDRdh
"""

!pip install yfinance prophet statsmodels scikit-learn torch seaborn matplotlib pmdarima

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
plt.rcParams["figure.figsize"] = (12, 4)
sns.set_theme()

ticker = "AAPL"

data = yf.download(ticker, start="2015-01-01", end="2024-12-31")

# If columns are MultiIndex (Price / Ticker), flatten them
if isinstance(data.columns, pd.MultiIndex):
    data.columns = [c[0] for c in data.columns]

data.head()

# Closing price trend
data['Close'].plot(title=f"{ticker} Closing Price")
plt.show()

# Volume trend
data['Volume'].plot(title="Trading Volume Trend")
plt.show()

# Daily returns
data['Returns'] = data['Close'].pct_change()
sns.histplot(data['Returns'].dropna(), bins=60, kde=True)
plt.title("Distribution of Daily Returns")
plt.show()

# Boxplot for outliers
sns.boxplot(x=data['Returns'].dropna())
plt.title("Returns – Outliers")
plt.show()

# Moving averages
data['SMA_20'] = data['Close'].rolling(20).mean()
data['SMA_50'] = data['Close'].rolling(50).mean()

data[['Close','SMA_20','SMA_50']].plot(title="Close with 20 & 50 Day SMA")
plt.show()

# Correlation heatmap
corr_cols = ['Close','High','Low','Open','Volume','SMA_20']
sns.heatmap(data[corr_cols].corr(), annot=False, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

df = data.dropna().copy()   # drop initial NaNs from SMA

close_series = df['Close']

test_horizon = 200
train = close_series[:-test_horizon]
test  = close_series[-test_horizon:]

train.index, test.index[:3], test.shape

def evaluate(actual, predicted, name):
    rmse = np.sqrt(mean_squared_error(actual, predicted))
    mae  = mean_absolute_error(actual, predicted)
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100

    print(f"\n{name} Performance:")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAE : {mae:.2f}")
    print(f"MAPE: {mape:.2f}%")
    return rmse, mae, mape

data['Volume'].plot(figsize=(12,4), title="Trading Volume Trend")

from pmdarima import auto_arima
from statsmodels.tsa.arima.model import ARIMA

# auto-select (p,d,q) on training close prices
stepwise_fit = auto_arima(
    train,
    start_p=1, start_q=1,
    max_p=5, max_q=5,
    seasonal=False,
    d=None,
    trace=True,
    error_action='ignore',
    suppress_warnings=True
)

best_order = stepwise_fit.order
print("Best ARIMA order:", best_order)

# fit final ARIMA on train
arima_model = ARIMA(train, order=best_order)
arima_fit = arima_model.fit()

# forecast the same horizon as test
arima_forecast = arima_fit.forecast(steps=len(test))

arima_scores = evaluate(test.values, arima_forecast.values, "ARIMA")

from prophet import Prophet

# Prophet expects a dataframe with 'ds' and 'y'
df_prophet = df[['Close']].reset_index().rename(columns={'Date':'ds','Close':'y'})

# Align train/test like before
df_train_prophet = df_prophet.iloc[:-test_horizon]
df_test_prophet  = df_prophet.iloc[-test_horizon:]

m = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=False,
    daily_seasonality=False
)
m.fit(df_train_prophet)

future = m.make_future_dataframe(periods=test_horizon)
forecast = m.predict(future)

# Take only the last 'test_horizon' points as prediction
prophet_pred = forecast['yhat'].iloc[-test_horizon:].values
actual = df_test_prophet['y'].values

prophet_scores = evaluate(actual, prophet_pred, "Prophet")

# Optional: full forecast plot
fig = m.plot(forecast)
plt.title(f"Prophet Forecast – {ticker}")
plt.show()

plt.figure(figsize=(12,5))
plt.plot(test.index, test.values, label="Actual", color="black")
plt.plot(test.index, arima_forecast.values, label="ARIMA Forecast", color="red")
plt.plot(test.index, prophet_pred, label="Prophet Forecast", color="blue")
plt.title("ARIMA vs Prophet – Test Period Forecast Comparison")
plt.xlabel("Date"); plt.ylabel("Price")
plt.legend()
plt.show()

import yfinance as yf
data = yf.download("AAPL", start="2015-01-01", end="2024-12-31")

# If MultiIndex columns from yfinance, flatten them
if isinstance(data.columns, pd.MultiIndex):
    data.columns = [c[0] for c in data.columns]

df = data.copy()

# --- Feature Engineering ---
# Daily returns
df['Return'] = df['Close'].pct_change()

# Moving averages
df['SMA_10'] = df['Close'].rolling(10).mean()
df['SMA_30'] = df['Close'].rolling(30).mean()

# Exponential moving averages
df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()
df['EMA_30'] = df['Close'].ewm(span=30, adjust=False).mean()

# Rolling volatility (std of returns)
df['Volatility_10'] = df['Return'].rolling(10).std()

# Simple RSI implementation
delta = df['Close'].diff()
gain = delta.where(delta > 0, 0.0)
loss = -delta.where(delta < 0, 0.0)
avg_gain = gain.rolling(14).mean()
avg_loss = loss.rolling(14).mean()
rs = avg_gain / (avg_loss + 1e-9)
df['RSI_14'] = 100 - (100 / (1 + rs))

# Drop initial NaNs from rolling calculations
df = df.dropna()

df.head()

feature_cols = [
    'Close', 'Return', 'SMA_10', 'SMA_30',
    'EMA_10', 'EMA_30', 'Volatility_10', 'RSI_14', 'Volume'
]

df_model = df[feature_cols].copy()

# Scale features and target with MinMaxScaler
feature_scaler = MinMaxScaler()
target_scaler = MinMaxScaler()

features_scaled = feature_scaler.fit_transform(df_model.values)
target_scaled = target_scaler.fit_transform(df[['Close']].loc[df_model.index].values)

SEQ_LEN = 30

def create_sequences(features, target, seq_len):
    X, y = [], []
    for i in range(len(features) - seq_len):
        X.append(features[i:i+seq_len])
        y.append(target[i+seq_len])   # next time step close
    return np.array(X), np.array(y)

X_all, y_all = create_sequences(features_scaled, target_scaled, SEQ_LEN)
X_all.shape, y_all.shape

test_size = 200
X_train, X_test = X_all[:-test_size], X_all[-test_size:]
y_train, y_test = y_all[:-test_size], y_all[-test_size:]

X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)
y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)
X_test_t  = torch.tensor(X_test, dtype=torch.float32).to(device)
y_test_t  = torch.tensor(y_test, dtype=torch.float32).to(device)

X_train_t.shape, y_train_t.shape

class LSTMForecaster(nn.Module):
    def __init__(self, n_features, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]      # last time step
        out = self.fc(out)
        return out

n_features = X_train.shape[2]
model = LSTMForecaster(n_features=n_features, hidden_size=64, num_layers=2).to(device)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

EPOCHS = 25
batch_size = 64

def train_lstm(model, X_train, y_train, epochs, batch_size):
    model.train()
    n = X_train.shape[0]
    for epoch in range(1, epochs+1):
        perm = torch.randperm(n)
        epoch_loss = 0.0

        for i in range(0, n, batch_size):
            idx = perm[i:i+batch_size]
            xb = X_train[idx]
            yb = y_train[idx]

            optimizer.zero_grad()
            preds = model(xb)
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * xb.size(0)

        epoch_loss /= n
        if epoch % 5 == 0 or epoch == 1:
            print(f"Epoch {epoch}/{epochs} - Loss: {epoch_loss:.6f}")

train_lstm(model, X_train_t, y_train_t, EPOCHS, batch_size)

model.eval()
with torch.no_grad():
    y_pred_test = model(X_test_t).cpu().numpy()

# Inverse scaling to get real prices
y_test_inv = target_scaler.inverse_transform(y_test)
y_pred_inv = target_scaler.inverse_transform(y_pred_test)

# Flatten
y_test_inv = y_test_inv.flatten()
y_pred_inv = y_pred_inv.flatten()

rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
mae  = mean_absolute_error(y_test_inv, y_pred_inv)
mape = np.mean(np.abs((y_test_inv - y_pred_inv) / y_test_inv)) * 100

print("LSTM Performance:")
print(f"RMSE: {rmse:.2f}")
print(f"MAE : {mae:.2f}")
print(f"MAPE: {mape:.2f}%")

# Plot
idx = df_model.index[-test_size:]  # last 200 dates
plt.figure(figsize=(12,5))
plt.plot(idx, y_test_inv, label='Actual', color='black')
plt.plot(idx, y_pred_inv, label='LSTM Forecast', color='orange')
plt.title("LSTM Forecast vs Actual (Last 200 Days)")
plt.xlabel("Date"); plt.ylabel("Price")
plt.legend()
plt.show()

