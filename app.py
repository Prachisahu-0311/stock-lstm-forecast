# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ycRHtcGsAR44JTKrK9qTvmLO99FcQ1pe
"""

import streamlit as st
import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error


from statsmodels.tsa.arima.model import ARIMA
from prophet import Prophet


# Utility Functions

def evaluate_metrics(actual, predicted):
    actual = np.array(actual)
    predicted = np.array(predicted)
    rmse = np.sqrt(mean_squared_error(actual, predicted))
    mae = mean_absolute_error(actual, predicted)
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100
    return rmse, mae, mape

def create_sequences(features, target, seq_len):
    X, y = [], []
    for i in range(len(features) - seq_len):
        X.append(features[i:i+seq_len])
        y.append(target[i+seq_len])
    return np.array(X), np.array(y)

class LSTMForecaster(nn.Module):
    def __init__(self, n_features, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = out[:, -1, :]
        out = self.fc(out)
        return out


# Streamlit App

def main():
    st.title("ðŸ“ˆ Stock Price Forecasting Dashboard â€“ ARIMA vs Prophet vs LSTM")
    st.write(
        "This app compares three time-series forecasting approaches on stock closing prices:\n"
        "- **ARIMA** (classical statistical model)\n"
        "- **Prophet** (additive trend/seasonality model)\n"
        "- **LSTM** (deep learning model with engineered technical indicators)"
    )

    # Sidebar config
    st.sidebar.header("Configuration")
    ticker = st.sidebar.text_input("Stock Ticker", value="AAPL")
    start_date = st.sidebar.date_input("Start Date", value=pd.to_datetime("2015-01-01"))
    end_date = st.sidebar.date_input("End Date", value=pd.to_datetime("today"))

    test_size = st.sidebar.slider("Test Size (days)", 60, 300, 200)
    seq_len = st.sidebar.slider("LSTM Lookback Window", 20, 60, 30)
    epochs = st.sidebar.slider("LSTM Training Epochs", 5, 40, 20, step=5)

    run_arima = st.sidebar.checkbox("Run ARIMA", value=True)
    run_prophet = st.sidebar.checkbox("Run Prophet", value=True)
    run_lstm = st.sidebar.checkbox("Run LSTM", value=True)

    if st.sidebar.button("Run Forecast"):
        if not (run_arima or run_prophet or run_lstm):
            st.warning("Please select at least one model to run.")
            return

        with st.spinner("Downloading data..."):
            data = yf.download(ticker, start=start_date, end=end_date)

        if data.empty:
            st.error("No data downloaded. Check ticker or date range.")
            return


        if isinstance(data.columns, pd.MultiIndex):
            data.columns = [c[0] for c in data.columns]

        df = data.copy()

        st.subheader("Raw Data Preview")
        st.write(df.tail())

        # plot with feature close
        st.subheader("Closing Price Trend")
        fig0, ax0 = plt.subplots(figsize=(10, 4))
        ax0.plot(df.index, df["Close"], label="Close")
        ax0.set_title(f"{ticker} Closing Price")
        ax0.set_xlabel("Date")
        ax0.set_ylabel("Price")
        ax0.legend()
        st.pyplot(fig0)


        if len(df) <= test_size + seq_len + 10:
            st.error("Not enough data for the chosen test size and lookback. Reduce test size or extend date range.")
            return


        # Preparation Close series for ARIMA / Prophet

        close_series = df["Close"].dropna()
        train_close = close_series[:-test_size]
        test_close = close_series[-test_size:]
        test_index = test_close.index

        # Dicts to store results
        model_predictions = {}
        model_metrics = {}


        # ARIMA IMPLEMENTATION

        if run_arima:
            st.subheader("ðŸ”¹ ARIMA Model")
            st.write("Using a fixed-order ARIMA(5,1,0) model for demonstration.")

            try:
                arima_model = ARIMA(train_close, order=(5, 1, 0))
                arima_fit = arima_model.fit()
                arima_forecast = arima_fit.forecast(steps=len(test_close))

                rmse, mae, mape = evaluate_metrics(test_close.values, arima_forecast.values)
                model_predictions["ARIMA"] = arima_forecast.values
                model_metrics["ARIMA"] = (rmse, mae, mape)

                st.write(f"**ARIMA RMSE:** {rmse:.2f}")
                st.write(f"**ARIMA MAE :** {mae:.2f}")
                st.write(f"**ARIMA MAPE:** {mape:.2f}%")

            except Exception as e:
                st.error(f"ARIMA failed: {e}")


        # Prophet IMPLEMENTATION

        if run_prophet:
            st.subheader("ðŸ”¹ Prophet Model")

            try:
                df_prophet = df[["Close"]].reset_index().rename(columns={"Date": "ds", "Close": "y"})
                # Align train/test splits like before
                df_train_prophet = df_prophet.iloc[:-test_size]
                df_test_prophet = df_prophet.iloc[-test_size:]

                m = Prophet(
                    yearly_seasonality=True,
                    weekly_seasonality=False,
                    daily_seasonality=False
                )
                m.fit(df_train_prophet)

                future = m.make_future_dataframe(periods=test_size)
                forecast = m.predict(future)

                prophet_pred = forecast["yhat"].iloc[-test_size:].values
                actual_prophet = df_test_prophet["y"].values

                rmse, mae, mape = evaluate_metrics(actual_prophet, prophet_pred)
                model_predictions["Prophet"] = prophet_pred
                model_metrics["Prophet"] = (rmse, mae, mape)

                st.write(f"**Prophet RMSE:** {rmse:.2f}")
                st.write(f"**Prophet MAE :** {mae:.2f}")
                st.write(f"**Prophet MAPE:** {mape:.2f}%")

            except Exception as e:
                st.error(f"Prophet failed: {e}")

       # LSTM IMPLEMENTATION
        if run_lstm:
            st.subheader("ðŸ”¹ LSTM Model with Technical Indicators")

            # Feature engineering
            df_fe = df.copy()
            df_fe["Return"] = df_fe["Close"].pct_change()
            df_fe["SMA_10"] = df_fe["Close"].rolling(10).mean()
            df_fe["SMA_30"] = df_fe["Close"].rolling(30).mean()
            df_fe["EMA_10"] = df_fe["Close"].ewm(span=10, adjust=False).mean()
            df_fe["EMA_30"] = df_fe["Close"].ewm(span=30, adjust=False).mean()
            df_fe["Volatility_10"] = df_fe["Return"].rolling(10).std()

            delta = df_fe["Close"].diff()
            gain = delta.where(delta > 0, 0.0)
            loss = -delta.where(delta < 0, 0.0)
            avg_gain = gain.rolling(14).mean()
            avg_loss = loss.rolling(14).mean()
            rs = avg_gain / (avg_loss + 1e-9)
            df_fe["RSI_14"] = 100 - (100 / (1 + rs))

            df_fe = df_fe.dropna()

            # Make sure we still have enough rows
            if len(df_fe) <= test_size + seq_len + 5:
                st.error("Not enough data after feature engineering for LSTM. Try reducing test size.")
            else:
                st.write("Engineered features preview:")
                st.write(df_fe.tail())

                feature_cols = [
                    "Close", "Return", "SMA_10", "SMA_30",
                    "EMA_10", "EMA_30", "Volatility_10", "RSI_14", "Volume"
                ]
                df_model = df_fe[feature_cols].copy()

                feature_scaler = MinMaxScaler()
                target_scaler = MinMaxScaler()

                features_scaled = feature_scaler.fit_transform(df_model.values)
                target_scaled = target_scaler.fit_transform(df_fe[["Close"]].values)

                X_all, y_all = create_sequences(features_scaled, target_scaled, seq_len)

                X_train = X_all[:-test_size]
                X_test = X_all[-test_size:]
                y_train = y_all[:-test_size]
                y_test = y_all[-test_size:]

                device = torch.device("cpu")
                X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)
                y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)
                X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)
                y_test_t = torch.tensor(y_test, dtype=torch.float32).to(device)

                n_features = X_train.shape[2]
                model = LSTMForecaster(n_features=n_features, hidden_size=64, num_layers=2).to(device)
                criterion = nn.MSELoss()
                optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

                n = X_train_t.shape[0]
                batch_size = 64

                st.write(f"Training LSTM for {epochs} epochs...")
                for epoch in range(1, epochs + 1):
                    model.train()
                    perm = torch.randperm(n)
                    epoch_loss = 0.0

                    for i in range(0, n, batch_size):
                        idx = perm[i:i + batch_size]
                        xb = X_train_t[idx]
                        yb = y_train_t[idx]

                        optimizer.zero_grad()
                        preds = model(xb)
                        loss = criterion(preds, yb)
                        loss.backward()
                        optimizer.step()
                        epoch_loss += loss.item() * xb.size(0)

                    epoch_loss /= n
                    if epoch % 5 == 0 or epoch == 1:
                        st.write(f"Epoch {epoch}/{epochs} - Loss: {epoch_loss:.6f}")

                model.eval()
                with torch.no_grad():
                    y_pred_test = model(X_test_t).cpu().numpy()

                y_test_inv = target_scaler.inverse_transform(y_test)
                y_pred_inv = target_scaler.inverse_transform(y_pred_test)

                y_test_inv = y_test_inv.flatten()
                y_pred_inv = y_pred_inv.flatten()

                rmse, mae, mape = evaluate_metrics(y_test_inv, y_pred_inv)
                model_predictions["LSTM"] = y_pred_inv
                model_metrics["LSTM"] = (rmse, mae, mape)

                st.write(f"**LSTM RMSE:** {rmse:.2f}")
                st.write(f"**LSTM MAE :** {mae:.2f}")
                st.write(f"**LSTM MAPE:** {mape:.2f}%")

        # Performance comparision plot

        if model_predictions:
            st.subheader("ðŸ“‰ Comparison: Actual vs Model Forecasts (Test Window)")
            fig, ax = plt.subplots(figsize=(12, 5))
            ax.plot(test_index, test_close.values[-test_size:], label="Actual Close", color="black")

            for name, preds in model_predictions.items():
                # Align lengths â€“ LSTM may be based on slightly shifted index, use last test_size indices
                ax.plot(test_index, preds[-test_size:], label=name)

            ax.set_xlabel("Date")
            ax.set_ylabel("Price")
            ax.set_title(f"{ticker} â€“ Model Forecast Comparison (Last {test_size} days)")
            ax.legend()
            st.pyplot(fig)

            # Metrics table
            st.subheader("ðŸ“Š Model Performance Summary")
            metrics_table = []
            for name, (rmse, mae, mape) in model_metrics.items():
                metrics_table.append({
                    "Model": name,
                    "RMSE": round(rmse, 2),
                    "MAE": round(mae, 2),
                    "MAPE (%)": round(mape, 2)
                })
            st.table(pd.DataFrame(metrics_table).set_index("Model"))

            st.success("Done! Models trained and compared.")

if __name__ == "__main__":
    main()
